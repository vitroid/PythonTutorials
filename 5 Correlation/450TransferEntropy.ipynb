{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸送エントロピー\n",
    "相互情報量は、2つの情報列に関して、相関関数(線形相関を取り出す)よりも広い意味での関連性を見付けだせることを紹介した。\n",
    "\n",
    "一方、我々が扱うデータは、単にデータが並んでいるだけではなく、時間的順序を持つ、いわゆる時系列データである場合がある。\n",
    "\n",
    "2つの時系列データ列$A(t)$と$B(t)$の間に、時間的なずれがあり、かつ線形相関がある場合には、時間相関関数でそれを見付けだすことができる。時間相関関数は、例えば次のようなものである。\n",
    "\\begin{equation}\n",
    "C_{AB}(\\Delta t)=\\sum_{t'}A(t')B(t'+\\Delta t)\n",
    "\\end{equation}\n",
    "連続的なデータの場合には総和の代わりに積分を使う。また、適宜規格化する。$A$と$B$が同じデータの場合は、自己相関関数と呼ばれる。ある地点で定常的に発生する信号が、$\\Delta t$だけ遅れて別の地点に伝わる場合、それぞれの地点での観測量$A$と$B$の時間相関関数を計算すれば、どれだけ遅れて伝わっているか、どれだけ情報が失われているか(ノイズが乗っているか)などを判断することができる。ただし，線形でない相関には対処できないのはPearson相関関数と同様である．\n",
    "\n",
    "相関関数と相互情報量の関係を、時系列データに拡張すると、時間相関関数に対応する、新たな情報量を定義できる。\n",
    "\n",
    "例えば、2つの時系列データ列$A_n$と$B_n$があり、どちらも定常的であると仮定する。(つまり、$A$のとる値の分布は、どの時刻でも同じ形になっているものとする。)また、取り扱いを簡単にするため、時刻は離散化されているものとする。(つまり、$n$は整数)\n",
    "\n",
    "時刻$n$での状態$A_n$、$B_n$の情報量$H(A_n,B_n)$と、次の時刻の状態$A_{n+1}$の情報量$H(A_{n+1})$の間の相互情報量$I(A_{n+1},A_n,B_n)$を考えてみよう。\n",
    "\\begin{eqnarray}\n",
    "I(A_{n+1},A_n,B_n)&=&H(A_{n+1})+H(A_n,B_n)-H(A_{n+1},A_n,B_n)\\\\\n",
    "&=&H(A_{n+1})-H(A_{n+1}|A_n,B_n)\\\\\n",
    "\\end{eqnarray}\n",
    "ところで、もし$A_{n+1}$と$B_n$が独立であれば、式(6.3)より第2項は単に$-H(A_{n+1}|A_n)$となるので、これらの差を求めると、 輸送エントロピー\\footnote{\\it Transfer Entropy}の定義式\n",
    "\\begin{equation}\n",
    "H(A_{n+1}|A_n)-H(A_{n+1}|A_n,B_n)=I(A_{n+1},B_n|A_n)\n",
    "\\end{equation}\n",
    "が得られる。\n",
    "\n",
    "この式は次のような意味をもつ。\n",
    "\n",
    "通常の信号の場合，現在の$A_n$と未来の$A_{n+1}$には関連性があるので、現在の$A_n$を知ることで、未来の$A_{n+1}$がどのような値になるかを、すこし正確に知ることができる(不確かさが減る)。その量は相互情報量$I(A_{n+1},A_n)=H(A_{n+1})-H(A_{n+1}|A_n)$で求められる。\n",
    "\n",
    "さらに、現在の$B_n$と未来の$A_{n+1}$に関連性がある場合には、現在の$B_n$を知ることで、未来の$A_{n+1}$がどのような値になるかを、さらに正確に知ることができる(不確かさが減る)。この差を、$B_n$から$A_{n+1}$への輸送エントロピー(あるいは輸送情報量)と呼ぶ[1]。\n",
    "\n",
    "$B_n$から$A_{n+1}$への輸送エントロピーと、$A_n$から$B_{n+1}$への輸送エントロピーは全く異なる量である。両者を見比べることで、情報の流れの上流がどちらかを知ることができる[2,3]。 \n",
    "\n",
    "[1] Schreiber, T.; Measuring information transfer. {\\it Physical Review Letters}, {\\bf 85(2)}, (2000) 461--464.\n",
    "\n",
    "[2] わかりやすいチュートリアル http://users.utu.fi/attenka/TEpresentation081128.pdf\n",
    "\n",
    "[3] MIC(最大情報係数)を、輸送エントロピーに拡張することもできる。 (を相互情報量の最大値で規格化したもの)を最大化するように、binサイズを調節すればいい。つまり、である。これを使うと、先験的な知識なしに、時間差を含むデータの関連性、情報の流れを見付けだすことができる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
