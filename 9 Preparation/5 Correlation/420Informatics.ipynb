{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情報理論の基礎\n",
    "統計力学を学んだ人から見ると、情報理論は「エントロピーに特化した統計力学」に見える。\n",
    "\n",
    "## 情報エントロピー\n",
    "まず，情報エントロピーを定義しよう。たとえば$i$の目がでる確率が $p_i$ であるサイコロを $N$ 回ふって、実際に$i$ の目が出た回数が $n_i$ 回であったとする。1の目がでる事象から6の目がでる事象までをあわせて，事象の集合$A$と書くことにする．(この文では，個別の事象については小文字を、事象の集合については大文字を使う。)\n",
    "\n",
    "そのような状況が実現される確率$P$は、順列の場合の数(多重度、あるいは状態数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "W &=& _NC_{n_i}\\\\\n",
    "&=&\\frac{N!}{(N-n_i)!n_i!}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と、個々の目がでる確率(尤度)\n",
    "\n",
    "\\begin{equation}\n",
    "L =\tp_i^{n_i}(1-p_i)^{N-n_i}\n",
    "\\end{equation}\n",
    "\n",
    "の積で、$P = WL$と書ける。\n",
    "\n",
    "$P$ が最も大きくなる(つまり、一番おこりやすそうな) $n_i$ の組み合わせは、変分法で求めることができて、$n_i = N p_i$ の場合、つまりそれぞれの目が均等にでる場合であることがわかる。(大数の法則)そして、$N$ が十分大きければ、$P$ は1になる。つまり、偏った目の出方がおこる確率はほぼ0になってしまう。これは、熱力学での平衡状態に対応する。\n",
    "\n",
    "一番おこりやすそうな$n_i$の組み合わせでの、状態数の対数(情報理論では、対数の底は2とする。)をとると、\n",
    "\n",
    "$$S=-\\sum_{i\\in A}Np_i\\log_2p_i$$\n",
    "\n",
    "同じサイコロをつかって、まず $N_1$ 回の試行 (= サイコロを振る) を行い、そのあと $N_2$ 回の試行を行うと、両方をあわせた状態数の対数$S$ は、足し算で計算できる。つまり、$S$ は示量変数である。そこで、$S$ を試行回数 $N$ で割ると、\n",
    "\n",
    "$$S / N = -\\sum_{i\\in A}p_i\\log_2p_i$$\n",
    "\n",
    "が得られる。この量は、試行一回あたりの、状態数の対数であり、情報エントロピーと呼ばれ、記号 $H(A)$であらわす。\n",
    "\n",
    "$$H = -\\sum_{i\\in A}p_i\\log_2p_i $$\n",
    "\n",
    "この量は、 $-\\log_2p_i$ の期待値の形になっていることがわかる。 $-\\log_2p_i$ のことを情報量と呼ぶので、情報エントロピーは、情報量の期待値、つまり平均情報量と言いかえられる。\n",
    "\n",
    "情報量とは、データの不確かさ(あるいは多様性)の尺度である。まず、4面サイコロと6面サイコロを比べると、両者の平均情報量(情報エントロピー)は$\\log 4$と$\\log 6$となる。つまり、情報量は、とりうる状態の数が多いほど増え、その大きさはとりうる状態の数の対数となる。独立な事象の同時確率が、それぞれの確率の積で表せるのと同じ理由で、独立な事象の情報量は、それぞれの情報量の和で表すことができる。(後述)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情報エントロピーの基本的な性質\n",
    "確率の基本定理から、情報エントロピーに関するいくつかの式を導ける。\n",
    "\n",
    "### 条件付き情報エントロピー\n",
    "確率の乗法定理から、情報エントロピー$H(A|B)$を求めてみよう。$H(A|B)$は，事象の集合$B$ のいずれかの事象$y$が起こったと知っている場合の、事象の集合$A$のいずれかの事象$x$が起こる確率の情報エントロピーである。注意しなければいけないのは、$H$ の中にあらわれる $A$ や $B$ は、特定の事象を表しているのではなく、とりうるすべての状態を表すということである。特定の状態を表す場合には小文字を、すべての状態を表す場合には大文字で書く。\n",
    "\n",
    "例えば、$B$ がある特定の状態$b$にある時の、$A$ の情報エントロピーは、情報量$\\log_2P(a|b)$の $A$ に関する期待値として、次のように書ける。\n",
    "\n",
    "$$H(A|b) =-\\sum_{a\\in A}P(a|b)\\log_2P(a|b)$$\n",
    "\n",
    "これを、さらにすべての $b$ について平均すると、情報エントロピー $H(A|B)$ は\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(A|B)&=&\\sum_{b\\in B}P(b)H(A|b)\\\\\n",
    "&=&-\\sum_{b\\in B}P(b)\\sum_{a\\in A}P(a|b)\\log_2P(a|b)\\\\\n",
    "&=&-\\sum_{b\\in B}\\sum_{a\\in A}P(a,b)\\log_2P(a|b)\n",
    "\\end{eqnarray}\n",
    "\n",
    "であることがわかる．\n",
    "\n",
    "ところで、B の情報エントロピーを、確率の加法定理を使ってAを含めて書きなおすと、\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(B)&=&-\\sum_{b\\in B}P(b)\\log_2P(b)\\\\\n",
    "&=&-\\sum_{b\\in B}\\sum_{a\\in A}P(a,b)\\log_2P(b)\n",
    "\\end{eqnarray}\n",
    "\n",
    "が得られる。A と B の同時確率の情報エントロピーは、これらを用いると、\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(A,B)&=&-\\sum_{b\\in B}\\sum_{a\\in A}P(a,b)\\log_2P(a,b)\\\\\n",
    "&=&-\\sum_{b\\in B}\\sum_{a\\in A}P(a,b)\\log_2[P(a|b)P(b)]\\\\\n",
    "&=&-\\sum_{b\\in B}\\sum_{a\\in A}[P(a,b)\\log_2P(a|b)+P(a,b)\\log_2P(b)]\\\\\n",
    "&=&H(A|B)+H(B)\n",
    "\\end{eqnarray}\n",
    "\n",
    "これは， 乗法定理(式\\ref{eq:prod})に対応する、情報量の公式である．\n",
    "\n",
    "もし、A と Bが独立なら、$P(a|b)=P(a)$なので、$H(A|B)=H(A)$となり，\n",
    "\n",
    "$$H(A,B)=H(A)+H(B)$$\n",
    "\n",
    "となる。独立な場合には、同時情報量(同時確率の情報エントロピー)は個々の情報量(情報エントロピー)の和になる。独立でなければ、同時情報量はこれよりも必ず小さくなる。情報量が、不確かさの尺度であったことを思いだしてほしい。独立でない場合には、不確かさが減る。独立であるとは、互いに全く無関係であるということであり、関係を知っていれば、片方の情報から相手の情報をより正確に予測できるので、不確かさは減る。一般には\n",
    "\n",
    "$$H(A,B)\\le H(A)+H(B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
