{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "## 相互情報量、独立性\n",
    "相互情報量は次の式で定義される．\n",
    "$$\n",
    "I(A,B)=H(A)+H(B)-H(A,B)\n",
    "$$\n",
    "\n",
    "これは、事象の集合$A$と$B$が独立でないために減少する情報量(不確かさ)である。相互情報量とは、「2つの情報源の間にどれだけ関連性があるか=独立でないか」の尺度であり、相互情報量が0であれば、$A$と$B$は独立、逆も真である。例えば、事象の集合$A$と$B$がどちらも2状態で、互いに独立であれば、独立事象の確率は個々の確率の積になるので、(Table \\ref{tbl:indy0})\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\caption{AとBが独立な場合．}\\label{tbl:indy0}\n",
    "\\begin{tabular}{|c|c||c|c||c|}\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{$I(A,B)=0$}&\\multicolumn{2}{|c||}{event B}&\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&TRUE&FALSE&\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "event A&TRUE&$P(A,B)=P(A)P(B)$&$P(A)P(\\bar B)$&$P(A)$\\\\ \\cline{2-5}\n",
    "&FALSE&$P(\\bar A)P(B)$&$P(\\bar A)P(\\bar B)$&$P(\\bar A)=1-P(A)$\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{}&$P(B)$&$P(\\bar B)$&\\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "この場合、相互情報量は0になる。 (簡単のため、真と偽の2状態としたが、もっと状態数が多くても同じ。)\n",
    "\\begin{eqnarray}\n",
    "I(A,B)&=&H(A)+H(B)-H(A,B)\\\\\n",
    "&=&-\\sum_{a\\in A}P(a)\\log P(a)-\\sum_{b\\in B}P(b)\\log P(b)+\\sum_{a\\in A}\\sum_{b\\in B}P(a,b)\\log P(a,b)\n",
    "\\end{eqnarray}\n",
    "$A$と$B$が独立なら，右辺第3項は，\t\n",
    "\\begin{eqnarray}\n",
    "\\sum_{a\\in A}\\sum_{b\\in B}P(a,b)\\log P(a,b)&=&\\sum_{a\\in A}\\sum_{b\\in B}P(a)P(b)\\log P(a)P(b)\\\\\n",
    "&=&\\sum_{a\\in A}\\sum_{b\\in B}P(a)P(b)\\log P(a)+\\sum_{a\\in A}\\sum_{b\\in B}P(a)P(b)\\log P(b)\\\\\n",
    "&=&\\sum_{a\\in A}P(a)\\log P(a)+\\sum_{b\\in B}P(b)\\log P(b)\n",
    "\\end{eqnarray}\n",
    "なので，たしかに$I(A,B)=0$になる．\n",
    "\n",
    "実際に，$A$と$B$が独立になるような数字を入れてみる。(Table \\ref{tbl:indy})\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\caption{AとBが独立な実例}\\label{tbl:indy}\n",
    "\\begin{tabular}{|c|c||c|c||c|c|}\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{I(A,B)=0}&\\multicolumn{2}{|c||}{event B}&\\multicolumn{2}{|l|}{}\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&TRUE&FALSE&\\multicolumn{2}{|l|}{}\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "event A&TRUE&0.42&0.28&0.7&$H(A)=0.881$\\\\ \\cline{2-5}\n",
    "&FALSE&0.18&0.12&0.3&\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{}&0.6&0.4&\\multicolumn{2}{|l|}{}\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&\\multicolumn{2}{|l||}{$H(B)=0.971$}&\\multicolumn{2}{|l|}{}\\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "では、逆に相互情報量を最大にするのはどんな場合だろう。相互情報量は、両者の情報が独立でなければないほど大きくなる。上の表で、同時確率をいじってみる．(Table \\ref{tbl:dy})\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\caption{Aとが独立でないケース．}\\label{tbl:dy}\n",
    "\\begin{tabular}{|c|c||c|c||c|c|}\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{$I(A,B)=0.557$}&\\multicolumn{2}{|c||}{event B}&\\multicolumn{2}{|l|}{}\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&TRUE&FALSE&\\multicolumn{2}{|l|}{}\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "event A&TRUE&0.6&0.1&0.7&$H(A)=0.881$\\\\ \\cline{2-5}\n",
    "&FALSE&0.0&0.3&0.3&\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{}&0.6&0.4&\\multicolumn{2}{|l|}{}\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&\\multicolumn{2}{|c||}{$H(B)=0.971$}&\\multicolumn{2}{|l|}{}\\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "$A$が真の場合には$B$も真になる場合が多い、という依存関係を強めると、相互情報量は0.557となった。\n",
    "\n",
    "$I(A,B)=H(A)+H(B)-H(A,B)$だから、$H(A)$や$H(B)$が大きく$H(A,B)$が小さいほど相互情報量は大きくなるだろう。$H(A)$は、$A$の各状態が均等に出現するほど大きくなる。上の例だと、$H(A)=0.881$、$H(B)=0.971$だが、$H$は真と偽がどちらも0.5の場合が最大で、その場合の情報量はちょうど1(単位はbit)となる。\n",
    "\n",
    "$H(A)$と$H(B)$が最大でかつ$A$と$B$が強く依存している($P(A,B)$が著しく偏っている)場合、相互情報量は最大値1(bit)となる。(Table \\ref{tbl:maxdy})\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\caption{相互情報量が最大となるケース．}\\label{tbl:maxdy}\n",
    "\\begin{tabular}{|c|c||c|c||c|c|}\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{$I(A,B)=1.0$}&\\multicolumn{2}{|c||}{event B}&\\multicolumn{2}{|l|}{}\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&TRUE&FALSE&\\multicolumn{2}{|l|}{}\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "event A&TRUE&0.5&0.0&0.5&$H(A)=1$\\\\ \\cline{2-5}\n",
    "&FALSE&0.0&0.5&0.5&\\\\\n",
    "\\hline\n",
    "\\hline\n",
    "\\multicolumn{2}{|l||}{}&0.5&0.5&\\multicolumn{2}{|l|}{}\\\\ \\cline{3-4}\n",
    "\\multicolumn{2}{|l||}{}&\\multicolumn{2}{|c||}{$H(B)=1$}&\\multicolumn{2}{|l|}{}\\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "一般に、$A$と$B$がどちらも $n$ 択(とりうる状態の数が$n$)の場合、$A$と$B$の相互情報量の最大値は $\\log_2 n$ になる。また、$A$が$n$ 択、$B$が $m$ 択の場合には、相互情報量の最大値は、$\\log_2 {\\rm min}(n, m)$となる。\n",
    "\n",
    "相互情報量は、線形相関かどうか、尺度量であるか否かに関係なく、2つの信号が従属していれば非0になる。相互情報量が非0であれば、片方の信号を知ることで、もう一方の信号を、高い信頼度で予測することができることを意味する。このため、信号の素性を知らなくても、関連性を判断できる点が長所である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
