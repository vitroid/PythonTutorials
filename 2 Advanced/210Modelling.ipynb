{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複雑性とモデル化\n",
    "物理学は、物事の理(ことわり)を究める学問であり、その目標は、物事の未来を予測することにある。現状を説明できるだけでなく、一を聞いて十を知ることが目的である。\n",
    "\n",
    "かつて素粒子物理学者は、大統一理論が完成した暁には、宇宙のすべてがわかると豪語した(少なくとも松本にはそう聞こえた)。その後、素粒子物理は大いに発展したが、素粒子物理がこの世のすべてを説明する見込みはまったくなく、むしろ「この世はわからないことだらけである」ということがわかってきた。\n",
    "\n",
    "確かに、量子性は不確定性をもたらし、統計力学によればエントロピーは増大して、系は常に乱雑になる方向に向かう。人間の観測にも誤差がともなう。しかし、不確定性がなかったとしても、エントロピーが増大しなくても、観測誤差を小さくする努力をしても、未来は一定の精度以上に予測することはできないことがはっきりしてきた。天気予報で明日の降水確率は正確に予測できても、一週間後の降水確率はごく粗い予測しかできない。\n",
    "\n",
    "その原因は、カオス性にある。カオス性とは、非線形微分方程式の解を求めるにあたり、初期の摂動が指数関数的に増幅される現象である。このため、初期状態を無限に正確に知ることができない限り、未来の状態には大きな誤差が生じる。困ったことに、カオスはある特殊な非線形微分方程式でのみ起こる現象ではない。また、統計力学が扱うような、自由度が非常に大きい(粒子数が多い)系で起こるわけでもなく、たった3体が相互作用する物体の運動でもカオスが生じる。\n",
    "\n",
    "カオスであっても、自由度が小さく、誤差が蓄積しない程度の未来であれば、予測することはできる。太陽系は多体力学系だが、この先100年で地球が惑星軌道から外れてしまう心配はしなくてもいい。自由度が大きくなるにつれ(カオス性が強まるにつれ)、予測できる範囲はせまくなっていく。大気の力学系ほどになると、明日の天気は予測できても、一週間先になるとかなりあやしくなる。一方、自由度が非常に大きく、強いカオスに支配される系は、事実上ランダムと見分けがつかないので、統計性がよく成り立ち、十分に長い時間の平均をとれば、統計学や統計力学でうまく扱える。\n",
    "\n",
    "問題は、決定論的な未来と、統計的な未来の間にある、「複雑な」未来を、どうやって理解し、どうやって予測するかである。きれいな数式で解析的に解が得られるケースは稀で、世の中のほとんどの現象はむしろ複雑であること、そして、その複雑性は、世界にたくさんの物があり、それらが非線形微分方程式に従って動いている以上、避けて通れないし、簡単にできる見込みもないことがカオスの研究によって明らかになってしまったのである。\n",
    "\n",
    "ただ、予測ができないといっても、力学的な予測ができないという意味であって、統計的な予測はできる。一週間後の天気が力学方程式から求められなくても、梅雨のさなかに、どんな確率で雨が降るかはわかる。\n",
    "\n",
    "どうせ正確に計算しても、未来が統計的にしか予測できないのなら、正確に計算する必要はない、と考えることもできる。気体分子を質点で表現しても気体の物性を再現できるなら、わざわざ気体分子の電子状態まで考えてリアルな相互作用を再現する必要はないだろう。現象をどのような側面でとらえて理解するかによって、必要な計算の正確さが違ってくる。\n",
    "\n",
    "このように考えていくと、真実であることと、人間が分かることのギャップが見えてくる。近似をすればするほど、気体分子の挙動は、真の気体分子とは似てもにつかぬものになるかもしれないが、それでもある側面は「わかる」し、より予測能力が高まる。逆にいえば、何をわかりたいか、という人間の都合で、必要な近似の度合いが決まる。\n",
    "\n",
    "近似することをモデリング、モデル化と呼ぶ。Lennard-Jonesのモデルは、気体のある性質をうまく近似し、TIP4Pは水のある側面を近似するモデルである。\n",
    "\n",
    "シミュレーションはすべてモデル化である。その近似の度合いは、人間が、現象をどのレベルでわかりたいか、何を予測したいかによって違ってくる。かつて、シミュレーションは、結果が予測できる物事を対象に行われていたが、現在のシミュレーションは、むしろ結果が予測できない物事が起こることを期待して行われる。\n",
    "\n",
    "モデル化とは近似であり、そこには人の主観が入っている。モデル化により、複雑な系を限られた変数で簡単に表現することで、モデルの適用範囲をせばめ、正確さを失うのとひきかえに、人間にとってのわかりやすさ、予測能力をもたらす。複雑なシステム、複雑なこの現実世界は、たくさんのモデルを集積することでしか理解できないのではないか、というのが、現代の複雑系研究者の共通の認識である。これを、ある人は「21世紀の科学は博物学に戻る」と表現し、ある人は「現代版の百科全書」と表現する。\n",
    "<!--\n",
    "はじめの頃の講義で紹介した、MICや輸送エントロピーは、複雑系に隠された「問題」を探りあてる手法だった。これに対し、モデル化はその「答」を導く(あるいは答がわからないままに応用する)ための手法と言える。\n",
    "-->\n",
    "\n",
    "\n",
    "最も単純なモデル化として、回帰(最小二乗法)があげられる。最小二乗法では、散在するデータ{x,y}に対し、真の関数y=f(x)と、そのまわりの誤差を仮定し、もっともありそうなf(x)を推定する。f(x)に直線関数が選ばれることが多いのは、それが最低次の多項式だからである。\n",
    "##多項式によるフィッティング\n",
    "学生実験では、測定データを直線でフィッティングし、その傾きを求めるのに最小二乗法を使った。最小二乗法は、原理的には、直線だけでなく、任意の多項式でのフィッティングに拡張できる。(原理的には、と書いたのは、次数が高い場合には、数値計算誤差が無視できなくなるので、計算順序に注意する必要がでてくるから)\n",
    "\n",
    "以下のようなデータを、多項式でフィットする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "file = open(\"data8.txt\",\"r\")\n",
    "xs = []\n",
    "ys = []\n",
    "for line in file:\n",
    "    cols = line.split()\n",
    "    if len(cols)>1:\n",
    "        x = float(cols[0])\n",
    "        y = float(cols[1])\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "plt.plot(xs,ys,\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直感的に、3次関数以上でないとうまくフィットできない気がする。実際に、1次から順にフィッティングをためしていこう。\n",
    "\n",
    "fittingにはscikit-learn(sklearn)ライブラリが使える。(sklearnは機械学習(人工知能の一種)のための膨大なライブラリ)\n",
    "\n",
    "(http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html)\n",
    "\n",
    "作業手順はとっつきにくいが、やっていることは多項式の係数を最小二乗法で決定(modelの生成)し、それをつかってxのほかの値でのyの値を予測(predict)している。predictをプロットすると、もとのデータをどれぐらい忠実になぞっているかがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = open(\"data8.txt\",\"r\")\n",
    "xs = []\n",
    "ys = []\n",
    "for line in file:\n",
    "    cols = line.split()\n",
    "    if len(cols) > 1:\n",
    "        x = float(cols[0])\n",
    "        y = float(cols[1])\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "plt.ylim(-2,2)\n",
    "\n",
    "plt.plot(xs,ys,\".\")\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X = xs[:, np.newaxis]            #列ベクトルに変換\n",
    "print(xs)\n",
    "print(X)\n",
    "x_plot = np.arange(-6,6,0.1)     #プロット用の、細かい間隔のデータ列。\n",
    "print(x_plot)\n",
    "X_plot = x_plot[:, np.newaxis]   #こちらも列ベクトルに変換する。\n",
    "\n",
    "for degree in [0, 1, 2, 4, 6, 10]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(X, ys)\n",
    "    y_plot = model.predict(X_plot)     #predictはX_plotで与えられた位置でのフィッティング関数の値を返す。\n",
    "    plt.plot(x_plot, y_plot, label=\"degree %d\" % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次数を上げれば上げるほど、データ点には近接していくものの、関数はのたうち回る。N点のデータを(N-1)次関数でフィットすれば、原理的にはすべての点を正確に通る(のたうちまわる)曲線が引けるはずだが、それは正しいモデルと言えるのだろうか。正しくないとすれば、いったい何次でフィットするのが良いのか。\n",
    "\n",
    "この測定値の背景にある、データを生みだすメカニズムがわかっていれば、「正しい」関数をつかってフィットすれば正しい係数が得られるはずだ。直線になるはずのデータを12次多項式でフィットするのがナンセンスなのは間違いない。しかし、今回の場合、データ点が何を意味しているのかわがからない以上、何次関数でモデル化すればいいのかはわからない。\n",
    "\n",
    "そこで、視点を変えてみる。モデルが正しいなら、データをいくつか間引いてフィットしても、その結果はロバストなはずだ。あるいは、いくつかのデータを間引いてフィットしても、フィットで得られる曲線は間引いたデータからは大きくはずれないはずだ。(間引くデータの集合を、確認用集合と呼ぶ)\n",
    "\n",
    "実際にこれをたしかめてみよう。11点のデータのうち1つ(x,y)をとりのぞいてフィットし、得られた多項式のxでの値f(x)とyがどれぐらい離れているかを測定してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = open(\"data8.txt\",\"r\")\n",
    "xL = []\n",
    "yL = []\n",
    "for line in file:\n",
    "    cols = line.split()\n",
    "    if len(cols)>1:\n",
    "        x = float(cols[0])\n",
    "        y = float(cols[1])\n",
    "        xL.append(x)\n",
    "        yL.append(y)\n",
    "\n",
    "#plt.plot(xs,ys,\".\")\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#xsとysはxLとyLのコピー。\n",
    "xs = xL.copy()\n",
    "ys = yL.copy()\n",
    "\n",
    "#xsの4番目のデータだけ取り出す。xsから4番目のデータが消える。\n",
    "x_target = xs.pop(4)\n",
    "y_target = ys.pop(4)\n",
    "print(xs)\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "X = xs[:, np.newaxis]            #列ベクトルに変換\n",
    "#知りたいのはx_targetでの値のみ。\n",
    "x_plot = np.array([[x_target]])\n",
    "\n",
    "for degree in range(10):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(X, ys)\n",
    "    y_predict = model.predict(x_plot)     #predictはX_plotで与えられた位置でのフィッティング関数の値を返す。\n",
    "    print(degree, y_target, y_predict[0])     #次数、測定値、フィット値\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11個中の4番目のデータを抜いた場合は、3次以上の何次関数でフィットしてもそれほど差はないように見える。しかし、4を1に書きかえると、次数が高いほど成績が悪くなることがわかる。そこで、0番目から10番目までをそれぞれ抜いた時の、測定値とフィット値の差の二乗和を計算してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file = open(\"data8.txt\",\"r\")\n",
    "xL = []\n",
    "yL = []\n",
    "for line in file:\n",
    "    cols = line.split()\n",
    "    if len(cols)>1:\n",
    "        x = float(cols[0])\n",
    "        y = float(cols[1])\n",
    "        xL.append(x)\n",
    "        yL.append(y)\n",
    "\n",
    "#plt.plot(xs,ys,\".\")\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#ループの順番をいれかえた。\n",
    "deg = []\n",
    "err = []\n",
    "for degree in range(10):\n",
    "    sum = 0.0\n",
    "    for N in range(len(xL)):\n",
    "\n",
    "        #xsとysはxLとyLのコピー。\n",
    "        xs = xL.copy()\n",
    "        ys = yL.copy()\n",
    "\n",
    "        #xsのN番目のデータだけ取り出す。xsからN番目のデータが消える。\n",
    "        x_target = xs.pop(N)\n",
    "        y_target = ys.pop(N)\n",
    "\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "\n",
    "        X = xs[:, np.newaxis]            #列ベクトルに変換\n",
    "        x_plot = np.array([[x_target]])\n",
    "\n",
    "        model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "        model.fit(X, ys)\n",
    "        y_predict = model.predict(x_plot)     #predictはX_plotで与えられた位置でのフィッティング関数の値を返す。\n",
    "        sum += (y_target - y_predict[0])**2     #測定値とフィット値の差の二乗を累積する。\n",
    "    print(degree,sum)\n",
    "    deg.append(degree)\n",
    "    err.append(sum)\n",
    "\n",
    "plt.ylim(0,10)\n",
    "plt.xlabel(\"highest degree in polynomial\")\n",
    "plt.ylabel(\"error in validation\")\n",
    "plt.plot(deg,err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果からは、3次関数でフィットすれば十分で、それ以上の次数でフィットしても信頼性は上がらないと言える。今回はデータの総数が少なかったので、確認用集合としてデータを1個だけ抜きとったが、データ数が多い場合には、もっとたくさんのデータを使うことができる。(cross-validation法; http://scikit-learn.org/stable/modules/cross_validation.html )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "加筆: Chevishev基底による多項式展開。フーリエ変換とセットでも良いと思う。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
