{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQ3LFk0M8ed"
      },
      "source": [
        "## Gaussian Mixture Model\n",
        "モデルがわかっている場合には、その関数でフィットして、パラメータを決めていくだけです。ここでは、よくある例として、ピークが複数ある分布を、複数のガウシアンの足しあわせで近似する場合を考えます。\n",
        "### 1次元"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYu-TzQoOISp"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/vitroid/PythonTutorials/master/2%20Advanced/data7.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UofyYwF5M8eg"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "#data7.txtは1カラムのデータ\n",
        "file = open(\"data7.txt\",\"r\")\n",
        "xL = []\n",
        "for line in file:\n",
        "    cols = line.split()\n",
        "    x = float(cols[0])\n",
        "    xL.append(x)\n",
        "\n",
        "xs = np.array(xL)\n",
        "X = xs[:, np.newaxis]            #列ベクトルに変換\n",
        "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)   #ヒストグラムでプロット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJROAQ08M8eh"
      },
      "source": [
        "3つのガウシアン分布でこの分布をフィットしてみる。(http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GMM.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar4_L91fM8eh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#3つのガウス関数でフィットする。\n",
        "model = GaussianMixture(3).fit(X)\n",
        "\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "x = x[:, np.newaxis]            #列ベクトルに変換\n",
        "y = np.exp(model.score_samples(x))      #推定された分布関数(scoreは対数を返すので、指数関数をかけている)\n",
        "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWPN7yOlM8eh"
      },
      "source": [
        "3つの成分に分けてみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEmlA8KoM8ei"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#3つのガウス関数でフィットする。\n",
        "model = GaussianMixture(3).fit(X)\n",
        "\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "x = x[:, np.newaxis]            #列ベクトルに変換\n",
        "y = np.exp(model.score_samples(x))      #推定された分布関数\n",
        "w = model.predict_proba(x)        #各Gaussianの\"重み\"\n",
        "y0 = w[:,0]*y\n",
        "y1 = w[:,1]*y\n",
        "y2 = w[:,2]*y\n",
        "\n",
        "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
        "plt.plot(x,y)\n",
        "plt.plot(x,y0)\n",
        "plt.plot(x,y1)\n",
        "plt.plot(x,y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNDfDtUWM8ei"
      },
      "source": [
        "重みだけをプロットしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKCdpSxwM8ei"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#3つのガウス関数でフィットする。\n",
        "model = GaussianMixture(3).fit(X)\n",
        "\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "x = x[:, np.newaxis]            #列ベクトルに変換\n",
        "y = np.exp(model.score(x))      #推定された分布関数\n",
        "w = model.predict_proba(x)      #各Gaussianの\"重み\"\n",
        "\n",
        "plt.plot(x,w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLLM_2O-M8ei"
      },
      "source": [
        "このグラフは何を意味しているのでしょうか。\n",
        "\n",
        "実験で得られた分布が、仮に3つのガウシアンの足しあわせでできているとしましょう。\n",
        "\n",
        "それはつまりこういうことです。測定している系は3つの独立な状態のいずれかに属しています。そして、状態1では、測定値は平均値-1のまわりで広い分布をもち、状態2では平均値0で分布し、状態3では平均値3のまわりで分布します。\n",
        "\n",
        "測定する人間には、測定結果しかわからないので、系がどの状態にあるのかはわかりません。\n",
        "\n",
        "もし、測定値が1だった時、系はどの状態にあると推定するのが一番合理的でしょうか。\n",
        "\n",
        "もちろん、状態2にあると推定すべきでしょう。なぜなら、x=1では、3つの状態のなかで、状態2が一番確率が高いからです。これが賭け事だとすれば、あなたは状態2に賭けるべきなのは明らかです。\n",
        "\n",
        "上のグラフの上側エンベロープは、あなたが賭けるべき状態を示しています。あるいは、一番もっともらしい、データの分類方法を示しているとも言えます。また、線が交わる点は、いちばんもっともらしい状態が入れかわる、「決定境界」を示しています。\n",
        "\n",
        "ただし、一つ心配な点があります。この分布は、本当に3つのガウシアン関数の足しあわせで良いのでしょうか。\n",
        "\n",
        "試しに、2つのガウシアンでフィットすると、あまり良くないことはわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeoYSLqQM8ej"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#3つのガウス関数でフィットする。\n",
        "model = GaussianMixture(2).fit(X)\n",
        "\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "x = x[:, np.newaxis]            #列ベクトルに変換\n",
        "y = np.exp(model.score_samples(x))      #推定された分布関数(scoreは対数を返すので、指数関数をかけている)\n",
        "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPbL0rbGM8ej"
      },
      "source": [
        "でも、4つのガウシアンでも問題ないようです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z2sxXuZM8ej",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#3つのガウス関数でフィットする。\n",
        "model = GaussianMixture(4).fit(X)\n",
        "\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "x = x[:, np.newaxis]            #列ベクトルに変換\n",
        "y = np.exp(model.score_samples(x))      #推定された分布関数(scoreは対数を返すので、指数関数をかけている)\n",
        "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jQrcwxPM8ej"
      },
      "source": [
        "では、最適な個数はいくつでしょうか。ここでも、あらかじめ3つの状態しかないことがわかっていれば良いのですが、それがわからない場合には困ってしまいます。ガウシアンの個数を増やせば、より忠実にヒストグラムのでこぼこに追従するようになりますが、それは本当に知りたい答とは限りません。\n",
        "\n",
        "一つの方法として、情報量規準を使う評価法があります。情報量規準は、統計モデルの良さを評価する指標です。一般に、モデルの次数を上げるほど、データとモデルの適合は良くなるが、過学習(過適合)に陥りがちになります。情報量規準はモデルの複雑さと、データの適合度のバランスをとるための「考え方」で、AIC, BICなど、いくつもの規準が提案されています。\n",
        "\n",
        "GaussianMixtureにはAIC(赤池情報量規準)とBIC(ベイズ情報量規準)があらかじめ準備されていますので、両方で最適なガウス関数の数を決めてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIzcAGkEM8ek"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "AIC = np.zeros(10)\n",
        "BIC = np.zeros(10)\n",
        "for n in range(1,10):\n",
        "    model = GaussianMixture(n).fit(X)\n",
        "    AIC[n] = model.aic(X)\n",
        "    BIC[n] = model.bic(X)\n",
        "    \n",
        "plt.plot(AIC, label=\"AIC\")\n",
        "plt.plot(BIC, label=\"BIC\")\n",
        "plt.xlim(1,9)\n",
        "plt.ylim(3800,4100)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok7tBQxyM8ek"
      },
      "source": [
        "評価は分かれました。AICは3つのガウシアンで十分だ、BICは、いや2でも3でも大差ない、という答になりました。いずれにしても、4つ以上のガウス関数を使う必要はなさそうです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyQOIRCiM8ek"
      },
      "source": [
        "### 2次元以上\n",
        "2次元以上の場合でも、ガウス分布の足しあわせに分解できます。\n",
        "\n",
        "https://github.com/scikit-learn/scikit-learn/blob/master/examples/mixture/plot_gmm_pdf.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atMsLYnSM8ek"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#Modified from:\n",
        "#https://github.com/scikit-learn/scikit-learn/blob/master/examples/mixture/plot_gmm_pdf.py\n",
        "\n",
        "#サンプルデータの生成\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_samples = 300\n",
        "\n",
        "# generate random sample, two components\n",
        "np.random.seed(0)\n",
        "\n",
        "# generate spherical data centered on (20, 20)\n",
        "shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n",
        "\n",
        "# generate zero centered stretched Gaussian data\n",
        "C = np.array([[0., -0.7], [3.5, .7]])\n",
        "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n",
        "\n",
        "# concatenate the two datasets into the final training set\n",
        "X_train = np.vstack([shifted_gaussian, stretched_gaussian])\n",
        "\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], .8)\n",
        "\n",
        "plt.axis('tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB37imnrM8el"
      },
      "outputs": [],
      "source": [
        "#複数の二次元ガウス関数でフィットする。\n",
        "from matplotlib.colors import LogNorm\n",
        "from sklearn import mixture\n",
        "\n",
        "# fit a Gaussian Mixture Model with two components\n",
        "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
        "clf.fit(X_train)\n",
        "\n",
        "# display predicted scores by the model as a contour plot\n",
        "x = np.linspace(-20.0, 30.0)\n",
        "y = np.linspace(-20.0, 40.0)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "XX = np.array([X.ravel(), Y.ravel()]).T\n",
        "Z = -clf.score_samples(XX)\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
        "                 levels=np.logspace(0, 3, 10))\n",
        "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], .8)\n",
        "\n",
        "plt.title('Negative log-likelihood predicted by a GMM')\n",
        "plt.axis('tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-DxvHsYLM8el"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "332GaussianMixture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
