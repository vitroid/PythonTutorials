{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク\n",
    "人工ニューラルネットワーク(ANN, 人工神経回路網)は、近年になって急激に脚光を浴びはじめた。\n",
    "\n",
    "1. **これまでの手法の限界**\n",
    "   教える(=認識する)内容が複雑になるにつれ、人によるパラメータチューニングに限界が見えてきた。\n",
    "\n",
    "2. **NNにおけるブレークスルー**\n",
    "   NN自体は1940年代から研究が始まっていたが、性能が伸び悩み、実用には向かないと考えられていた。今世紀に入りそれを打破する新しいアイディアにより、人の手助けなしに、自律的に極めて高度な学習ができるようになった。\n",
    "   \n",
    "3. **データ量の増大**\n",
    "   デジタル写真や音声データが一般化し、膨大な教師データを準備できるようになった。\n",
    "   \n",
    "4. **計算機性能の向上**\n",
    "   膨大な量のトレーニングができるようになった。\n",
    "   \n",
    "## ANNとは何か\n",
    "神経細胞(ニューロン)の信号伝達をモデル化し、それを大量に組みあわせてネットワーク化して脳の信号処理を模倣する試み。脳機能を忠実に再現しているわけではない。\n",
    "\n",
    "## 何ができるのか\n",
    "画像や音声などの、大きなデータを入力し、分類、エンコーディング、認識、タグ付け、ノイズ除去などのさまざまなタスクを行う。最新のディープニューラルネットワークを用いると、画像認識、文字認識等で人間を越える能力を発揮する。\n",
    "\n",
    "## ディープラーニングとは何か\n",
    "ディープニューラルネットワークを用いた機械学習。\n",
    "\n",
    "## ディープニューラルネットワークとは何か\n",
    "ネットワークの階層性が深いニューラルネットワークのこと。2016年現在、「非常に深い」ニューラルネットワークとは10層程度を指していたが、2018年にはすでに何十層もあるのがあたりまえになりつつある。基本的な構造はどのニューラルネットワークも同じだが、ディープニューラルネットワークを設計する場合には、各階層に役割を与える場合が多い。\n",
    "\n",
    "## もてはやされる理由\n",
    "\n",
    "1. 人力では達成不可能な複雑なタスクを実現できるようになった。\n",
    "\n",
    "2. 学習過程での計算量は莫大だが、一旦学習したあとは、かけ算と足し算の組みあわせによる、わずかな計算しか必要としない = スマホ程度の計算能力で十分利用できる。\n",
    "\n",
    "## 我々科学者の立ち位置\n",
    "1. DNNの開発に参入するのは難しい。計算機シミュレーションのために、半導体設計から始めるようなもの。よほどの勝算がなければやめておいたほうがいい。\n",
    "2. DNNで何ができ、何ができないかを理解した上で、DNNに適合するような問題設定を考える。既存のフレームワーク(TensorFlowなど)を使って自力で解くか、外部専門家に適切な要望を出して解決してもらう。\n",
    "\n",
    "## フレームワーク\n",
    "フレームワークとは、ニューラルネットワークを計算機に実装する一連の作業に必要なツール一式のこと。フレームワークが違うと、重みやバイアスといった変数をどんな形式で指定するか、バックプロパゲーションをどのように実施するか、などといった作業手順が違ってくる。\n",
    "\n",
    "適切なフレームワークを選べば、計算機の性能を最大限にひきだせる。複数のGPUを使って計算を加速したり、外部のCloudに任せてしまうことすら可能。(例えばAmazonのクラウドAWSはMXNetやTensorFlowに対応しているらしい)\n",
    "\n",
    "(July 2018)\n",
    "\n",
    "|FW name | 公開年   | 開発主体 | 言語  |github|\n",
    "|:---:|:---:|:------|:-----|:---|\n",
    "|Theano |(2010) |MontrealU |Python|[Theano/Theano](https://github.com/Theano/Theano)\n",
    "|Caffe   |2013     |UCB      |C++, Python|[BVLC/caffe](https://github.com/BVLC/caffe)\n",
    "|Chainer |2015     |PFN      |Python |[chainer/chainer](https://github.com/chainer/chainer)\n",
    "|CNTK    |2015      |Microsoft |Python|[Microsoft/CNTK](https://github.com/Microsoft/CNTK)\n",
    "|TensorFlow | 2015 | Google | C++, Python |[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)\n",
    "|neon    |2015   |Intel     |Python|[NervanaSystems/neon](https://github.com/NervanaSystems/neon)\n",
    "|PyTorch |2016   |NYU, Facebook|Python|[pytorch/pytorch](https://github.com/pytorch/pytorch)\n",
    "|NNabla  |2017   |Sony      |C++, Python   |[sony/nnabla](https://github.com/sony/nnabla)\n",
    "|Caffe2  |2017     |Facebook |C++, Python|[caffe2/caffe2](https://github.com/caffe2/caffe2)\n",
    "|MXNet   |2017     |WU, CMU  |Python, R, Julia, Go|[apache/incubator-mxnet](https://github.com/apache/incubator-mxnet)\n",
    "\n",
    "* Pythonが書ければ、どれも使える。\n",
    "* どのプロジェクトも、[github](https://github.com)でソースプログラムを公開している。\n",
    "* https://qiita.com/bonotake/items/cbd44abbcbe333f264d8 などを参考に整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlowによる実装例\n",
    "\n",
    "TensorFlowはGoogleが提供しているNNフレームワーク。次のサイトで教師あり学習の利用例をブラウザ上で試すことができる: https://playground.tensorflow.org\n",
    "\n",
    "ここでは、https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners をなぞってみる。\n",
    "\n",
    "MNISTは教師あり学習のための手書き数字のデータセットである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1段だけのNNを準備する (遅延評価)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価関数の定義 (遅延評価)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),\n",
    "                                              reduction_indices=[1]))\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練方法の定義 (訓練データによる評価→バックプロパゲーション。遅延評価)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練セッションを作る。\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数を初期化する\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練セッションの実行: 訓練データで1000回訓練する。\n",
    "for _ in range(100):\n",
    "    # 100組のランダムなデータを選ぶ\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(1000)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能評価関数の定義 (遅延評価)：\n",
    "# y_ (正解)とy (推定)の、それぞれの最高確率のものが一致するかどうか\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度関数の定義 (遅延評価)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証セッションの実行\n",
    "print(sess.run(accuracy, \n",
    "               feed_dict={x: mnist.test.images,\n",
    "                          y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータセットでの正答率は90%。悪い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能改善\n",
    "どうすれば良いか?\n",
    "* 階層を増やす。\n",
    "* 階層を増やすと過学習に陥る可能性がある。\n",
    "  * トレーニングを増やす。\n",
    "  * dropout(いくつかのリンクをランダムに外し、外乱に強いrobustなニューラルネットワークに育てる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# SoftMaxの代わりにReLUを使う場合、Biasはすこし与えたほうが良い。\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                            logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  if i%1000 == 0:\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))  \n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 従来法との比較\n",
    "\n",
    "同じことを決定木で試してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape # 60000枚の28x28のグレースケール画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape # 10000枚の28x28のグレースケール画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train # 正解(数字)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28x28=784次元は可視化できないので、決定木の生成はライブラリを信じるしかない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 分類器を準備する\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 画像を1次元のデータ列に読みかえる。\n",
    "x_train = x_train.reshape(60000,28*28)\n",
    "x_test  = x_test.reshape(10000,28*28)\n",
    "\n",
    "#まず教師データで学習させる\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#その結果を使ってtestデータを分類してエラーを測定する。\n",
    "ypred = model.predict(x_test)\n",
    "\n",
    "ypred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率88%。\n",
    "\n",
    "次はRandomForestを使ってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "\n",
    "# 分類器を準備する\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# 画像を1次元のデータ列に読みかえる。\n",
    "x_train = x_train.reshape(60000,28*28)\n",
    "x_test  = x_test.reshape(10000,28*28)\n",
    "\n",
    "#まず教師データで学習させる\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#その結果を使ってtestデータを分類してエラーを測定する。\n",
    "ypred = model.predict(x_test)\n",
    "\n",
    "ypred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正答率95%。これでもかなり性能が良いとは思うが、このへんがDeep Learning以前の限界に近い。ちなみに人間の性能は95〜98%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerasによる実装\n",
    "\n",
    "TensorFlowをそのまま使うとあまり読みやすくないので、Kerasでwrapしてみる。KerasはNeural Networkフレームワークのためのwrapperで、いろんなところから提供され、使い勝手がそれぞれ違うフレームワークをくるんで、差を吸収してくれる便利なライブラリ。異なるフレームワークを比較して自分に最適なものを選ぶのにも便利。\n",
    "\n",
    "* https://qiita.com/cvusk/items/2cd3e516276b426bc58c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networkのトポロジー定義\n",
    "# Kerasのサンプルから切り貼り。もともとは数字用ではないかもしれない。\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# 隠れ層が2枚あるNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conv2Dは畳み込み層(隣のピクセルだけから次の層の値を算出する)\n",
    "* MaxPoolingはプーリング層(最大値だけを選んで粗視化)\n",
    "* Denseは全結合層\n",
    "* dropoutは結合を間引いた全結合層(ランダムに結合を間引くことで「打たれ強い」(過学習になりにくい)ネットワークを作る)\n",
    "* Flattenは平滑化層。\n",
    "\n",
    "これを見ると、このネットワークは12層の中間層を持つ深いNNであることがよみとれる。どんな層をどんな順番で組み立てるかが性能を左右するが、それ自体も機械学習で最適化させるべき対象だろう。個々の層の役割についてはNeural Networkの教科書かネットの情報を参照してほしい。\n",
    "\n",
    "データの量もかなり多い(60000個×784ピクセル)が、最適化すべきパラメータの数(33万)も多い。複雑さという点では決定木をはるかにしのぐと言える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyterのセルの最初の行に`%%time`と書いておくと、そのセルを実行するのに要した時間を表示してくれる。\n",
    "\n",
    "以下が学習プロセスだが、実行するとたぶん講義の時間程度では終わらない。(松本の4 Coreマシンで4時間)\n",
    "\n",
    "帰って電源につないでから試してみてほしい。研究室に計算専用機やGPU搭載機があるなら、それで実行したほうが良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "てきとうにサンプルから切り貼りした(最適化してない)DNNでも99.1%が達成された。人間をはるかに凌ぐ性能!\n",
    "\n",
    "もはや特別な知識をもたなくても、画像を認識し、人間以上の精度で識別することが可能になっている。\n",
    "\n",
    "あなたの研究で、どんな側面にこれを利用できるだろうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
